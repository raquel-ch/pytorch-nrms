{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "epoch = 0\n",
    "num_epochs = 10\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from _constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "\n",
    "from _behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from _articles import convert_text2encoding_with_transformers\n",
    "from _polars import concat_str_columns, slice_join_dataframes\n",
    "from _articles import create_article_id_to_value_mapping\n",
    "from _nlp import get_transformers_word_embeddings\n",
    "from _python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from dataloader import NRMSDataLoader\n",
    "from model_config import hparams_nrms\n",
    "from NRMSModel import NRMSModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels\n",
    "We sample a few just to get started. For testset we just make up a dummy column with 0 and 1 - this is not the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"~/ebnerd_data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "DUMP_DIR = PATH.joinpath(\"downloads1\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we sample the dataset, just to keep it smaller. Also, one can simply add the testset similary to the validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>list[i8]</td></tr></thead><tbody><tr><td>712557</td><td>[9769909, 9770023, … 9769893]</td><td>[9778168, 9778168, … 9778192]</td><td>[9778220]</td><td>490667248</td><td>[0, 0, … 0]</td></tr><tr><td>2560566</td><td>[9769471, 9770882, … 9770989]</td><td>[9769370, 9432542, … 9099237]</td><td>[9769433]</td><td>464264700</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ u32     ┆ list[i32]         ┆ ---               ┆ ---              ┆ u32           ┆ list[i8]    │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 712557  ┆ [9769909,         ┆ [9778168,         ┆ [9778220]        ┆ 490667248     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9770023, …        ┆ 9778168, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9769893]          ┆ 9778192]          ┆                  ┆               ┆             │\n",
       "│ 2560566 ┆ [9769471,         ┆ [9769370,         ┆ [9769433]        ┆ 464264700     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9770882, …        ┆ 9432542, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9770989]          ┆ 9099237]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n",
    "HISTORY_SIZE = 10\n",
    "FRACTION = 0.01\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var i…</td><td>&quot;Politiet frygt…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den ø…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars …</td><td>&quot;Biografgængern…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har …</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natascha… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind for…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(\"ebnerd_small/articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=8,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using device: cuda:0\n",
      "torch.Size([250002, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_19836\\3535455046.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word2vec_embedding = torch.tensor(word2vec_embedding, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSModel(\n",
      "  (news_encoder): NewsEncoder(\n",
      "    (embedding): Embedding(250002, 768)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (multihead_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (attention): AttLayer2()\n",
      "  )\n",
      "  (candidate_encoder): NewsEncoder(\n",
      "    (embedding): Embedding(250002, 768)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (multihead_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (attention): AttLayer2()\n",
      "  )\n",
      "  (user_encoder): UserEncoder(\n",
      "    (news_encoder): NewsEncoder(\n",
      "      (embedding): Embedding(250002, 768)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (multihead_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (attention): AttLayer2()\n",
      "    )\n",
      "    (multihead_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (additive_attention): AttLayer2()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "Epoch: 1/100\n",
      "Loss: 8637.6220436096\n",
      "AUC: 0.4967442357\n",
      "--------------------------\n",
      "\n",
      "Epoch: 2/100\n",
      "Loss: 8631.7837162018\n",
      "AUC: 0.5200683177\n",
      "--------------------------\n",
      "\n",
      "Epoch: 3/100\n",
      "Loss: 8562.1957702637\n",
      "AUC: 0.5631938514\n",
      "--------------------------\n",
      "\n",
      "Epoch: 4/100\n",
      "Loss: 8525.8663387299\n",
      "AUC: 0.5728010248\n",
      "--------------------------\n",
      "\n",
      "Epoch: 5/100\n",
      "Loss: 8459.3870563507\n",
      "AUC: 0.6045046968\n",
      "--------------------------\n",
      "\n",
      "Epoch: 6/100\n",
      "Loss: 8400.1369686127\n",
      "AUC: 0.6116567037\n",
      "--------------------------\n",
      "\n",
      "Epoch: 7/100\n",
      "Loss: 8348.0116596222\n",
      "AUC: 0.6263877028\n",
      "--------------------------\n",
      "\n",
      "Epoch: 8/100\n",
      "Loss: 8356.4945945740\n",
      "AUC: 0.6233988044\n",
      "--------------------------\n",
      "\n",
      "Epoch: 9/100\n",
      "Loss: 8328.4098606110\n",
      "AUC: 0.6316182750\n",
      "--------------------------\n",
      "\n",
      "Epoch: 10/100\n",
      "Loss: 8298.2231769562\n",
      "AUC: 0.6328992314\n",
      "--------------------------\n",
      "\n",
      "Epoch: 11/100\n",
      "Loss: 8296.0985431671\n",
      "AUC: 0.6386635354\n",
      "--------------------------\n",
      "\n",
      "Epoch: 12/100\n",
      "Loss: 8299.8149394989\n",
      "AUC: 0.6415456874\n",
      "--------------------------\n",
      "\n",
      "Epoch: 13/100\n",
      "Loss: 8278.8481121063\n",
      "AUC: 0.6456020495\n",
      "--------------------------\n",
      "\n",
      "Epoch: 14/100\n",
      "Loss: 8277.6265678406\n",
      "AUC: 0.6461357814\n",
      "--------------------------\n",
      "\n",
      "Epoch: 15/100\n",
      "Loss: 8251.5032176971\n",
      "AUC: 0.6511528608\n",
      "--------------------------\n",
      "\n",
      "Epoch: 16/100\n",
      "Loss: 8252.4351425171\n",
      "AUC: 0.6521135781\n",
      "--------------------------\n",
      "\n",
      "Epoch: 17/100\n",
      "Loss: 8238.1945228577\n",
      "AUC: 0.6530742955\n",
      "--------------------------\n",
      "\n",
      "Epoch: 18/100\n",
      "Loss: 8221.7653408051\n",
      "AUC: 0.6653501281\n",
      "--------------------------\n",
      "\n",
      "Epoch: 19/100\n",
      "Loss: 8212.8188533783\n",
      "AUC: 0.6568637916\n",
      "--------------------------\n",
      "\n",
      "Epoch: 20/100\n",
      "Loss: 8232.1001815796\n",
      "AUC: 0.6594790777\n",
      "--------------------------\n",
      "\n",
      "Epoch: 21/100\n",
      "Loss: 8202.8265628815\n",
      "AUC: 0.6672181896\n",
      "--------------------------\n",
      "\n",
      "Epoch: 22/100\n",
      "Loss: 8183.7678451538\n",
      "AUC: 0.6737830914\n",
      "--------------------------\n",
      "\n",
      "Epoch: 23/100\n",
      "Loss: 8193.3260726929\n",
      "AUC: 0.6691929974\n",
      "--------------------------\n",
      "\n",
      "Epoch: 24/100\n",
      "Loss: 8187.2676124573\n",
      "AUC: 0.6706874466\n",
      "--------------------------\n",
      "\n",
      "Epoch: 25/100\n",
      "Loss: 8162.5701866150\n",
      "AUC: 0.6808283518\n",
      "--------------------------\n",
      "\n",
      "Epoch: 26/100\n",
      "Loss: 8133.6247024536\n",
      "AUC: 0.6877668659\n",
      "--------------------------\n",
      "\n",
      "Epoch: 27/100\n",
      "Loss: 8106.7008438110\n",
      "AUC: 0.6894748079\n",
      "--------------------------\n",
      "\n",
      "Epoch: 28/100\n",
      "Loss: 8114.0768718719\n",
      "AUC: 0.6906490179\n",
      "--------------------------\n",
      "\n",
      "Epoch: 29/100\n",
      "Loss: 8110.2847347260\n",
      "AUC: 0.6905956447\n",
      "--------------------------\n",
      "\n",
      "Epoch: 30/100\n",
      "Loss: 8073.4983062744\n",
      "AUC: 0.6928906917\n",
      "--------------------------\n",
      "\n",
      "Epoch: 31/100\n",
      "Loss: 8073.5721244812\n",
      "AUC: 0.6982280102\n",
      "--------------------------\n",
      "\n",
      "Epoch: 32/100\n",
      "Loss: 8071.8354091644\n",
      "AUC: 0.6999359522\n",
      "--------------------------\n",
      "\n",
      "Epoch: 33/100\n",
      "Loss: 8047.2588653564\n",
      "AUC: 0.7023911187\n",
      "--------------------------\n",
      "\n",
      "Epoch: 34/100\n",
      "Loss: 8048.3022975922\n",
      "AUC: 0.7014304014\n",
      "--------------------------\n",
      "\n",
      "Epoch: 35/100\n",
      "Loss: 8025.1846542358\n",
      "AUC: 0.7043125534\n",
      "--------------------------\n",
      "\n",
      "Epoch: 36/100\n",
      "Loss: 8028.8787059784\n",
      "AUC: 0.7077818104\n",
      "--------------------------\n",
      "\n",
      "Epoch: 37/100\n",
      "Loss: 7997.4175357819\n",
      "AUC: 0.7130123826\n",
      "--------------------------\n",
      "\n",
      "Epoch: 38/100\n",
      "Loss: 7995.9898052216\n",
      "AUC: 0.7176024765\n",
      "--------------------------\n",
      "\n",
      "Epoch: 39/100\n",
      "Loss: 7990.3573055267\n",
      "AUC: 0.7149338173\n",
      "--------------------------\n",
      "\n",
      "Epoch: 40/100\n",
      "Loss: 7976.9263019562\n",
      "AUC: 0.7163748933\n",
      "--------------------------\n",
      "\n",
      "Epoch: 41/100\n",
      "Loss: 7968.7369346619\n",
      "AUC: 0.7234201537\n",
      "--------------------------\n",
      "\n",
      "Epoch: 42/100\n",
      "Loss: 7968.0485591888\n",
      "AUC: 0.7235269001\n",
      "--------------------------\n",
      "\n",
      "Epoch: 43/100\n",
      "Loss: 7930.7558784485\n",
      "AUC: 0.7295580700\n",
      "--------------------------\n",
      "\n",
      "Epoch: 44/100\n",
      "Loss: 7948.3531646729\n",
      "AUC: 0.7277967549\n",
      "--------------------------\n",
      "\n",
      "Epoch: 45/100\n",
      "Loss: 7920.2162189484\n",
      "AUC: 0.7362830914\n",
      "--------------------------\n",
      "\n",
      "Epoch: 46/100\n",
      "Loss: 7919.4737014771\n",
      "AUC: 0.7335610589\n",
      "--------------------------\n",
      "\n",
      "Epoch: 47/100\n",
      "Loss: 7900.5146999359\n",
      "AUC: 0.7409799317\n",
      "--------------------------\n",
      "\n",
      "Epoch: 48/100\n",
      "Loss: 7918.3390312195\n",
      "AUC: 0.7374573015\n",
      "--------------------------\n",
      "\n",
      "Epoch: 49/100\n",
      "Loss: 7908.7975616455\n",
      "AUC: 0.7415670367\n",
      "--------------------------\n",
      "\n",
      "Epoch: 50/100\n",
      "Loss: 7888.0541763306\n",
      "AUC: 0.7386315115\n",
      "--------------------------\n",
      "\n",
      "Epoch: 51/100\n",
      "Loss: 7878.9313011169\n",
      "AUC: 0.7456767720\n",
      "--------------------------\n",
      "\n",
      "Epoch: 52/100\n",
      "Loss: 7864.8955612183\n",
      "AUC: 0.7487190436\n",
      "--------------------------\n",
      "\n",
      "Epoch: 53/100\n",
      "Loss: 7839.5937786102\n",
      "AUC: 0.7549637062\n",
      "--------------------------\n",
      "\n",
      "Epoch: 54/100\n",
      "Loss: 7835.0416469574\n",
      "AUC: 0.7543766012\n",
      "--------------------------\n",
      "\n",
      "Epoch: 55/100\n",
      "Loss: 7855.6888942719\n",
      "AUC: 0.7559244236\n",
      "--------------------------\n",
      "\n",
      "Epoch: 56/100\n",
      "Loss: 7826.6209430695\n",
      "AUC: 0.7594470538\n",
      "--------------------------\n",
      "\n",
      "Epoch: 57/100\n",
      "Loss: 7814.7172355652\n",
      "AUC: 0.7610482494\n",
      "--------------------------\n",
      "\n",
      "Epoch: 58/100\n",
      "Loss: 7789.3426456451\n",
      "AUC: 0.7637702818\n",
      "--------------------------\n",
      "\n",
      "Epoch: 59/100\n",
      "Loss: 7803.7410984039\n",
      "AUC: 0.7684671221\n",
      "--------------------------\n",
      "\n",
      "Epoch: 60/100\n",
      "Loss: 7810.7413139343\n",
      "AUC: 0.7678266439\n",
      "--------------------------\n",
      "\n",
      "Epoch: 61/100\n",
      "Loss: 7783.7350692749\n",
      "AUC: 0.7673996584\n",
      "--------------------------\n",
      "\n",
      "Epoch: 62/100\n",
      "Loss: 7769.0845489502\n",
      "AUC: 0.7777540564\n",
      "--------------------------\n",
      "\n",
      "Epoch: 63/100\n",
      "Loss: 7761.0752239227\n",
      "AUC: 0.7763663535\n",
      "--------------------------\n",
      "\n",
      "Epoch: 64/100\n",
      "Loss: 7766.8460941315\n",
      "AUC: 0.7699615713\n",
      "--------------------------\n",
      "\n",
      "Epoch: 65/100\n",
      "Loss: 7752.2188510895\n",
      "AUC: 0.7755657558\n",
      "--------------------------\n",
      "\n",
      "Epoch: 66/100\n",
      "Loss: 7761.5224933624\n",
      "AUC: 0.7746050384\n",
      "--------------------------\n",
      "\n",
      "Epoch: 67/100\n",
      "Loss: 7755.5291233063\n",
      "AUC: 0.7766332195\n",
      "--------------------------\n",
      "\n",
      "Epoch: 68/100\n",
      "Loss: 7754.4678878784\n",
      "AUC: 0.7788748933\n",
      "--------------------------\n",
      "\n",
      "Epoch: 69/100\n",
      "Loss: 7730.3810234070\n",
      "AUC: 0.7810631939\n",
      "--------------------------\n",
      "\n",
      "Epoch: 70/100\n",
      "Loss: 7737.9151134491\n",
      "AUC: 0.7794619983\n",
      "--------------------------\n",
      "\n",
      "Epoch: 71/100\n",
      "Loss: 7712.2606697083\n",
      "AUC: 0.7916844577\n",
      "--------------------------\n",
      "\n",
      "Epoch: 72/100\n",
      "Loss: 7698.7182006836\n",
      "AUC: 0.7925384287\n",
      "--------------------------\n",
      "\n",
      "Epoch: 73/100\n",
      "Loss: 7687.4960289001\n",
      "AUC: 0.7953672075\n",
      "--------------------------\n",
      "\n",
      "Epoch: 74/100\n",
      "Loss: 7729.2233047485\n",
      "AUC: 0.7867207515\n",
      "--------------------------\n",
      "\n",
      "Epoch: 75/100\n",
      "Loss: 7702.1256732941\n",
      "AUC: 0.7904568745\n",
      "--------------------------\n",
      "\n",
      "Epoch: 76/100\n",
      "Loss: 7706.8956890106\n",
      "AUC: 0.7952070880\n",
      "--------------------------\n",
      "\n",
      "Epoch: 77/100\n",
      "Loss: 7697.3531074524\n",
      "AUC: 0.7925384287\n",
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m nrms(his_input_title, pred_input_title)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), labels\u001b[38;5;241m.\u001b[39mfloat())  \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Apply gradient clipping\u001b[39;00m\n\u001b[0;32m     42\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(nrms\u001b[38;5;241m.\u001b[39mparameters(), max_norm)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.utils  # Ensure this is imported for gradient clipping\n",
    "\n",
    "epoch = 0\n",
    "num_epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "word2vec_embedding = torch.tensor(word2vec_embedding, dtype=torch.float32).to(device)\n",
    "print(word2vec_embedding.shape)\n",
    "\n",
    "nrms = NRMSModel(hparams_nrms=hparams_nrms, word2vec_embedding=word2vec_embedding, seed=42).to(device)  # Adding to device\n",
    "print(nrms)\n",
    "\n",
    "optimizer = torch.optim.Adam(nrms.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Gradient clipping parameter\n",
    "max_norm = 5.0  # Maximum gradient norm\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    nrms.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    for i, ((his_input_title, pred_input_title), labels) in enumerate(train_dataloader):\n",
    "        his_input_title = his_input_title.to(device, dtype=torch.long)\n",
    "        pred_input_title = pred_input_title.to(device, dtype=torch.long)\n",
    "        og_labels = labels\n",
    "        labels = labels.to(device, dtype=torch.long).view(-1)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = nrms(his_input_title, pred_input_title)  # Forward pass\n",
    "        \n",
    "        loss = loss_fn(outputs.view(-1), labels.float())  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        \n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(nrms.parameters(), max_norm)\n",
    "        total_norm = 0\n",
    "        for p in nrms.parameters():\n",
    "            if p.grad is not None:  # Only consider parameters with gradients\n",
    "                param_norm = p.grad.data.norm(2)  # Compute the L2 norm of gradients\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5  # Final gradient norm\n",
    "\n",
    "        optimizer.step()  # Update the parameters\n",
    "        running_loss += loss.item()\n",
    "        # if i % 50 == 0:\n",
    "        #     print(f\"Epoch {epoch + 1}, Batch {i + 1}: Gradient Norm = {total_norm:.4f}\")\n",
    "        #     print(f\"running_loss: {running_loss}\")\n",
    "        #     print(f\"outputs: {outputs}\")\n",
    "        \n",
    "        all_labels.extend(og_labels.cpu().numpy())\n",
    "        all_outputs.extend(outputs.detach().cpu().numpy())\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc = 0\n",
    "    for i, label_true in enumerate(all_labels):\n",
    "        auc += roc_auc_score(label_true, all_outputs[i])\n",
    "    auc /= len(all_labels)\n",
    "    \n",
    "    # Print epoch details\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Loss: {running_loss:.10f}\")\n",
    "    print(f\"AUC: {auc:.10f}\")\n",
    "    print(f\"--------------------------\\n\")\n",
    "\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    \n",
    "\n",
    "    # Detach tensors immediately after use to save memory\n",
    "    his_input_title = his_input_title.detach()\n",
    "    pred_input_title = pred_input_title.detach()\n",
    "    labels = labels.detach()\n",
    "    outputs = outputs.detach()\n",
    "    del his_input_title, pred_input_title, labels, outputs, loss\n",
    "    torch.cuda.empty_cache()  # Clear unused GPU memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 597s 8s/step - loss: 1.5952 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"NRMS\"\n",
    "# LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "# MODEL_WEIGHTS = f\"downloads/data/state_dict/{MODEL_NAME}/weights\"\n",
    "\n",
    "# # CALLBACKS\n",
    "# # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "# # early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "# # modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "# #     filepath=MODEL_WEIGHTS, save_best_only=True, save_weights_only=True, verbose=1\n",
    "# # )\n",
    "\n",
    "# hparams_nrms.history_size = HISTORY_SIZE\n",
    "\n",
    "\n",
    "# model = NRMSModel(\n",
    "#     hparams=hparams_nrms,\n",
    "#     word2vec_embedding=word2vec_embedding,\n",
    "#     seed=42,\n",
    "# )\n",
    "# hist = model.model.fit(\n",
    "#     train_dataloader,\n",
    "#     validation_data=val_dataloader,\n",
    "#     epochs=1,\n",
    "#     # callbacks=[tensorboard_callback, early_stopping, modelcheckpoint],\n",
    "# )\n",
    "# Uncomment the following line if you have pre-trained weights\n",
    "# _ = model.model.load_weights(filepath=MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 383s 2s/step\n"
     ]
    }
   ],
   "source": [
    "# pred_validation = model.scorer.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the predictions to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>scores</th><th>is_known_user</th><th>ranked_scores</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>bool</td><td>list[i64]</td></tr></thead><tbody><tr><td>386380</td><td>[9779867, 9779423, … 9779956]</td><td>[9787931, 9780460, … 9765777]</td><td>[9787465]</td><td>43657242</td><td>[0, 0, … 0]</td><td>[0.456477, 0.528305, … 0.508259]</td><td>true</td><td>[35, 13, … 23]</td></tr><tr><td>177925</td><td>[9777910, 9777769, … 9779511]</td><td>[8560195, 9486080, … 9790091]</td><td>[9789922]</td><td>110129832</td><td>[0, 0, … 0]</td><td>[0.520847, 0.433627, … 0.570924]</td><td>false</td><td>[3, 6, … 1]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 9)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ article_i ┆ article_i ┆ … ┆ labels    ┆ scores    ┆ is_known_ ┆ ranked_sc │\n",
       "│ ---     ┆ _fixed     ┆ ds_inview ┆ ds_clicke ┆   ┆ ---       ┆ ---       ┆ user      ┆ ores      │\n",
       "│ u32     ┆ ---        ┆ ---       ┆ d         ┆   ┆ list[i8]  ┆ list[f64] ┆ ---       ┆ ---       │\n",
       "│         ┆ list[i32]  ┆ list[i32] ┆ ---       ┆   ┆           ┆           ┆ bool      ┆ list[i64] │\n",
       "│         ┆            ┆           ┆ list[i32] ┆   ┆           ┆           ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 386380  ┆ [9779867,  ┆ [9787931, ┆ [9787465] ┆ … ┆ [0, 0, …  ┆ [0.456477 ┆ true      ┆ [35, 13,  │\n",
       "│         ┆ 9779423, … ┆ 9780460,  ┆           ┆   ┆ 0]        ┆ ,         ┆           ┆ … 23]     │\n",
       "│         ┆ 9779956]   ┆ …         ┆           ┆   ┆           ┆ 0.528305, ┆           ┆           │\n",
       "│         ┆            ┆ 9765777]  ┆           ┆   ┆           ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.508259] ┆           ┆           │\n",
       "│ 177925  ┆ [9777910,  ┆ [8560195, ┆ [9789922] ┆ … ┆ [0, 0, …  ┆ [0.520847 ┆ false     ┆ [3, 6, …  │\n",
       "│         ┆ 9777769, … ┆ 9486080,  ┆           ┆   ┆ 0]        ┆ ,         ┆           ┆ 1]        │\n",
       "│         ┆ 9779511]   ┆ …         ┆           ┆   ┆           ┆ 0.433627, ┆           ┆           │\n",
       "│         ┆            ┆ 9790091]  ┆           ┆   ┆           ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.570924] ┆           ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "#     add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    "# )\n",
    "# df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.523276170069743,\n",
       "    \"mrr\": 0.3271357380208631,\n",
       "    \"ndcg@5\": 0.368758819449155,\n",
       "    \"ndcg@10\": 0.44821698782392533\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics = MetricEvaluator(\n",
    "#     labels=df_validation[\"labels\"].to_list(),\n",
    "#     predictions=df_validation[\"scores\"].to_list(),\n",
    "#     metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    "# )\n",
    "# metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>scores</th><th>is_known_user</th><th>ranked_scores</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>bool</td><td>list[i64]</td></tr></thead><tbody><tr><td>386380</td><td>[9779867, 9779423, … 9779956]</td><td>[9787931, 9780460, … 9765777]</td><td>[9787465]</td><td>43657242</td><td>[0, 0, … 0]</td><td>[0.456477, 0.528305, … 0.508259]</td><td>true</td><td>[35, 13, … 23]</td></tr><tr><td>177925</td><td>[9777910, 9777769, … 9779511]</td><td>[8560195, 9486080, … 9790091]</td><td>[9789922]</td><td>110129832</td><td>[0, 0, … 0]</td><td>[0.520847, 0.433627, … 0.570924]</td><td>false</td><td>[3, 6, … 1]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 9)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ article_i ┆ article_i ┆ … ┆ labels    ┆ scores    ┆ is_known_ ┆ ranked_sc │\n",
       "│ ---     ┆ _fixed     ┆ ds_inview ┆ ds_clicke ┆   ┆ ---       ┆ ---       ┆ user      ┆ ores      │\n",
       "│ u32     ┆ ---        ┆ ---       ┆ d         ┆   ┆ list[i8]  ┆ list[f64] ┆ ---       ┆ ---       │\n",
       "│         ┆ list[i32]  ┆ list[i32] ┆ ---       ┆   ┆           ┆           ┆ bool      ┆ list[i64] │\n",
       "│         ┆            ┆           ┆ list[i32] ┆   ┆           ┆           ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 386380  ┆ [9779867,  ┆ [9787931, ┆ [9787465] ┆ … ┆ [0, 0, …  ┆ [0.456477 ┆ true      ┆ [35, 13,  │\n",
       "│         ┆ 9779423, … ┆ 9780460,  ┆           ┆   ┆ 0]        ┆ ,         ┆           ┆ … 23]     │\n",
       "│         ┆ 9779956]   ┆ …         ┆           ┆   ┆           ┆ 0.528305, ┆           ┆           │\n",
       "│         ┆            ┆ 9765777]  ┆           ┆   ┆           ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.508259] ┆           ┆           │\n",
       "│ 177925  ┆ [9777910,  ┆ [8560195, ┆ [9789922] ┆ … ┆ [0, 0, …  ┆ [0.520847 ┆ false     ┆ [3, 6, …  │\n",
       "│         ┆ 9777769, … ┆ 9486080,  ┆           ┆   ┆ 0]        ┆ ,         ┆           ┆ 1]        │\n",
       "│         ┆ 9779511]   ┆ …         ┆           ┆   ┆           ┆ 0.433627, ┆           ┆           │\n",
       "│         ┆            ┆ 9790091]  ┆           ┆   ┆           ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.570924] ┆           ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_validation = df_validation.with_columns(\n",
    "#     pl.col(\"scores\")\n",
    "#     .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "#     .alias(\"ranked_scores\")\n",
    "# )\n",
    "# df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the validation, simply add the testset to your flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 23282.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads\\predictions.txt to downloads\\predictions.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# write_submission_file(\n",
    "#     impression_ids=df_validation[DEFAULT_IMPRESSION_ID_COL],\n",
    "#     prediction_scores=df_validation[\"ranked_scores\"],\n",
    "#     path=\"downloads/predictions.txt\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningNew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
