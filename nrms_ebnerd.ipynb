{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "epoch = 0\n",
    "num_epochs = 10\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from _constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "\n",
    "from _behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from _articles import convert_text2encoding_with_transformers\n",
    "from _polars import concat_str_columns, slice_join_dataframes\n",
    "from _articles import create_article_id_to_value_mapping\n",
    "from _nlp import get_transformers_word_embeddings\n",
    "from _python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from dataloader import NRMSDataLoader\n",
    "from model_config import hparams_nrms\n",
    "from NRMSModel import NRMSModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters():\n",
    "    learning_rate = float(input(\"Learning rate: \"))\n",
    "    batch_size = int(input(\"Batch size: \"))\n",
    "    epochs = int(input(\"Epochs: \"))\n",
    "    weight_decay = float(input(\"Weight decay: \"))\n",
    "    head_dim = int(input(\"Head dimension/number: \"))\n",
    "    \n",
    "    return learning_rate, batch_size, epochs, weight_decay, head_dim\n",
    "\n",
    "learning_rate, batch_size, epochs, weight_decay, head_dim = get_parameters()\n",
    "hparams_nrms.learning_rate = learning_rate\n",
    "hparams_nrms.batch_size = batch_size\n",
    "hparams_nrms.epochs = epochs\n",
    "hparams_nrms.weight_decay = weight_decay\n",
    "hparams_nrms.head_dim = head_dim\n",
    "hparams_nrms.head_num = head_dim\n",
    "\n",
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels\n",
    "We sample a few just to get started. For testset we just make up a dummy column with 0 and 1 - this is not the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"~/ebnerd_data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "DUMP_DIR = PATH.joinpath(\"downloads1\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we sample the dataset, just to keep it smaller. Also, one can simply add the testset similary to the validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>list[i8]</td></tr></thead><tbody><tr><td>215444</td><td>[9766042, 9766338, … 9768649]</td><td>[9754160, 9775484, … 9775500]</td><td>[9754160]</td><td>342788370</td><td>[1, 0, … 0]</td></tr><tr><td>641293</td><td>[9770146, 9770741, … 9770390]</td><td>[9768583, 9770283, … 9771333]</td><td>[9771224]</td><td>307286914</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ u32     ┆ list[i32]         ┆ ---               ┆ ---              ┆ u32           ┆ list[i8]    │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 215444  ┆ [9766042,         ┆ [9754160,         ┆ [9754160]        ┆ 342788370     ┆ [1, 0, … 0] │\n",
       "│         ┆ 9766338, …        ┆ 9775484, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9768649]          ┆ 9775500]          ┆                  ┆               ┆             │\n",
       "│ 641293  ┆ [9770146,         ┆ [9768583,         ┆ [9771224]        ┆ 307286914     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9770741, …        ┆ 9770283, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9770390]          ┆ 9771333]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n",
    "HISTORY_SIZE = 10\n",
    "FRACTION = 0.01\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var i…</td><td>&quot;Politiet frygt…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den ø…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars …</td><td>&quot;Biografgængern…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har …</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natascha… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind for…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(\"ebnerd_small/articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250037, 384)\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "print(word2vec_embedding.shape)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=hparams_nrms.batch_size,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=hparams_nrms.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "Learning rate: 0.001\n",
      "Batch size: 8\n",
      "Epochs: 50\n",
      "Weight decay: 1e-07\n",
      "Head dimension: 8\n",
      "Head number: 8\n"
     ]
    }
   ],
   "source": [
    "def print_hparams(hparams):\n",
    "    print(\"Hyperparameters:\")\n",
    "    print(f\"Learning rate: {hparams.learning_rate}\")\n",
    "    print(f\"Batch size: {hparams.batch_size}\")\n",
    "    print(f\"Epochs: {hparams.epochs}\")\n",
    "    print(f\"Weight decay: {hparams.weight_decay}\")\n",
    "    print(f\"Head dimension: {hparams.head_dim}\")\n",
    "    print(f\"Head number: {hparams.head_num}\")\n",
    "    \n",
    "        \n",
    "print_hparams(hparams_nrms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSModel(\n",
      "  (embedding): Embedding(250037, 384)\n",
      "  (candidate_encoder): NewsEncoder(\n",
      "    (embedding): Embedding(250037, 384)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (multihead_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "    )\n",
      "    (attention): AttLayer2()\n",
      "  )\n",
      "  (user_encoder): UserEncoder(\n",
      "    (news_encoder): NewsEncoder(\n",
      "      (embedding): Embedding(250037, 384)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (multihead_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "      )\n",
      "      (attention): AttLayer2()\n",
      "    )\n",
      "    (multihead_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "    )\n",
      "    (additive_attention): AttLayer2()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (batch_norm_attention): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Parameter: embedding.weight, Requires Grad: False, Shape: torch.Size([250037, 384])\n",
      "Parameter: candidate_encoder.embedding.weight, Requires Grad: False, Shape: torch.Size([250037, 384])\n",
      "Parameter: candidate_encoder.multihead_attention.in_proj_weight, Requires Grad: True, Shape: torch.Size([1152, 384])\n",
      "Parameter: candidate_encoder.multihead_attention.in_proj_bias, Requires Grad: True, Shape: torch.Size([1152])\n",
      "Parameter: candidate_encoder.multihead_attention.out_proj.weight, Requires Grad: True, Shape: torch.Size([384, 384])\n",
      "Parameter: candidate_encoder.multihead_attention.out_proj.bias, Requires Grad: True, Shape: torch.Size([384])\n",
      "Parameter: candidate_encoder.attention.W, Requires Grad: True, Shape: torch.Size([384, 384])\n",
      "Parameter: candidate_encoder.attention.b, Requires Grad: True, Shape: torch.Size([384])\n",
      "Parameter: candidate_encoder.attention.q, Requires Grad: True, Shape: torch.Size([384, 1])\n",
      "Parameter: user_encoder.multihead_attention.in_proj_weight, Requires Grad: True, Shape: torch.Size([1152, 384])\n",
      "Parameter: user_encoder.multihead_attention.in_proj_bias, Requires Grad: True, Shape: torch.Size([1152])\n",
      "Parameter: user_encoder.multihead_attention.out_proj.weight, Requires Grad: True, Shape: torch.Size([384, 384])\n",
      "Parameter: user_encoder.multihead_attention.out_proj.bias, Requires Grad: True, Shape: torch.Size([384])\n",
      "Parameter: user_encoder.additive_attention.W, Requires Grad: True, Shape: torch.Size([384, 384])\n",
      "Parameter: user_encoder.additive_attention.b, Requires Grad: True, Shape: torch.Size([384])\n",
      "Parameter: user_encoder.additive_attention.q, Requires Grad: True, Shape: torch.Size([384, 1])\n",
      "Parameter: user_encoder.batch_norm_attention.weight, Requires Grad: True, Shape: torch.Size([384])\n",
      "Parameter: user_encoder.batch_norm_attention.bias, Requires Grad: True, Shape: torch.Size([384])\n",
      "Epoch: 1/50\n",
      "Training loss: 8671.8833484650, Training AUC: 0.5117954740\n",
      "Training outputs: [array([0.1999772 , 0.19999504, 0.1999983 , 0.20002294, 0.20000651],\n",
      "      dtype=float32), array([0.20000097, 0.19998486, 0.19999857, 0.20000596, 0.20000969],\n",
      "      dtype=float32), array([0.20000125, 0.2000029 , 0.20000125, 0.19998316, 0.20001137],\n",
      "      dtype=float32), array([0.20000285, 0.20000663, 0.19999596, 0.19999649, 0.199998  ],\n",
      "      dtype=float32), array([0.20000541, 0.19998883, 0.2000071 , 0.20000201, 0.1999967 ],\n",
      "      dtype=float32), array([0.20000006, 0.19999899, 0.19999252, 0.19999982, 0.20000854],\n",
      "      dtype=float32), array([0.2000063 , 0.19998963, 0.2000032 , 0.19999874, 0.20000213],\n",
      "      dtype=float32), array([0.19999062, 0.19999774, 0.20000142, 0.19999976, 0.20001045],\n",
      "      dtype=float32), array([0.20002475, 0.20001139, 0.19999582, 0.19998442, 0.19998361],\n",
      "      dtype=float32), array([0.19997205, 0.19999395, 0.19999695, 0.19996664, 0.20007038],\n",
      "      dtype=float32)]\n",
      "Training labels: [array([1., 0., 0., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32)]\n",
      "Validation outputs: [0.05264728, 0.0969588, 0.102831624, 0.0897422, 0.10592173, 0.09727412, 0.11628118, 0.10941453, 0.109702446, 0.094645135]\n",
      "Validation labels: [array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([1.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]\n",
      "Validation loss: 26.7523422241, Validation AUC: 0.5155523173\n",
      "--------------------------\n",
      "\n",
      "Epoch: 2/50\n",
      "Training loss: 8640.3028297424, Training AUC: 0.5051238258\n",
      "Training outputs: [array([0.19451226, 0.1679469 , 0.19651535, 0.16271444, 0.27831104],\n",
      "      dtype=float32), array([0.24332748, 0.2087738 , 0.24387337, 0.0763261 , 0.22769926],\n",
      "      dtype=float32), array([0.281032  , 0.08584946, 0.11005921, 0.235483  , 0.2875763 ],\n",
      "      dtype=float32), array([0.16905841, 0.258421  , 0.26764753, 0.06752726, 0.23734583],\n",
      "      dtype=float32), array([0.17913367, 0.23535937, 0.14934476, 0.12471816, 0.31144404],\n",
      "      dtype=float32), array([0.2471587 , 0.18253021, 0.12641312, 0.36350882, 0.08038914],\n",
      "      dtype=float32), array([0.25368053, 0.09265138, 0.17860551, 0.17813738, 0.29692522],\n",
      "      dtype=float32), array([0.12095056, 0.17917185, 0.23033766, 0.3058119 , 0.16372798],\n",
      "      dtype=float32), array([0.16114023, 0.19271007, 0.23227586, 0.15988463, 0.2539892 ],\n",
      "      dtype=float32), array([0.15502858, 0.10173108, 0.41788152, 0.1815907 , 0.1437681 ],\n",
      "      dtype=float32)]\n",
      "Training labels: [array([1., 0., 0., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32)]\n",
      "Validation outputs: [0.0013836601, 0.3391945, 0.34124142, 0.39311507, 0.3435609, 0.379048, 0.3269766, 0.32136962, 0.34661558, 0.33739528]\n",
      "Validation labels: [array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([1.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]\n",
      "Validation loss: 26.8964481354, Validation AUC: 0.4836302084\n",
      "--------------------------\n",
      "\n",
      "Epoch: 3/50\n",
      "Training loss: 8630.3447017670, Training AUC: 0.5190008540\n",
      "Training outputs: [array([0.18692142, 0.18360746, 0.18179727, 0.21456692, 0.23310694],\n",
      "      dtype=float32), array([0.20349026, 0.20297156, 0.2054128 , 0.19379014, 0.19433528],\n",
      "      dtype=float32), array([0.31394857, 0.00440585, 0.35275078, 0.01614524, 0.31274953],\n",
      "      dtype=float32), array([0.23789342, 0.17061956, 0.18963079, 0.19718611, 0.2046701 ],\n",
      "      dtype=float32), array([0.19084333, 0.20860694, 0.21234505, 0.19320676, 0.19499797],\n",
      "      dtype=float32), array([0.24641916, 0.24312265, 0.00101807, 0.2808445 , 0.2285956 ],\n",
      "      dtype=float32), array([0.22087802, 0.18591051, 0.23110193, 0.17638984, 0.18571976],\n",
      "      dtype=float32), array([0.18940169, 0.18761453, 0.22180536, 0.19032529, 0.21085313],\n",
      "      dtype=float32), array([0.19722322, 0.17619295, 0.19276161, 0.24341227, 0.19040991],\n",
      "      dtype=float32), array([0.22120298, 0.20336317, 0.17216346, 0.19056647, 0.21270397],\n",
      "      dtype=float32)]\n",
      "Training labels: [array([1., 0., 0., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32)]\n",
      "Validation outputs: [1.1080458e-08, 6.363727e-08, 8.701349e-09, 1.2460138e-08, 6.844555e-09, 2.2498407e-08, 2.3080624e-08, 1.107528e-08, 1.1767627e-08, 4.3246466e-08]\n",
      "Validation labels: [array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([1.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]\n",
      "Validation loss: 26.7954483032, Validation AUC: 0.4810095888\n",
      "--------------------------\n",
      "\n",
      "Epoch: 4/50\n",
      "Training loss: 8626.2434787750, Training AUC: 0.5234842015\n",
      "Training outputs: [array([0.03528704, 0.89644426, 0.04629396, 0.01871344, 0.00326131],\n",
      "      dtype=float32), array([0.06428216, 0.09743182, 0.2931546 , 0.3429064 , 0.20222507],\n",
      "      dtype=float32), array([0.06116916, 0.77309614, 0.06289433, 0.01183401, 0.09100629],\n",
      "      dtype=float32), array([0.22103582, 0.60319155, 0.0929786 , 0.01110319, 0.07169087],\n",
      "      dtype=float32), array([0.21088979, 0.59797734, 0.1212818 , 0.04220121, 0.0276499 ],\n",
      "      dtype=float32), array([0.3946203 , 0.11739592, 0.3725175 , 0.04824163, 0.06722465],\n",
      "      dtype=float32), array([0.30027217, 0.17827767, 0.05773066, 0.32504028, 0.13867922],\n",
      "      dtype=float32), array([0.06982227, 0.23542891, 0.060914  , 0.2849418 , 0.348893  ],\n",
      "      dtype=float32), array([0.05878191, 0.2921128 , 0.07725883, 0.1073048 , 0.4645416 ],\n",
      "      dtype=float32), array([0.29600924, 0.01020713, 0.03571811, 0.3828913 , 0.27517414],\n",
      "      dtype=float32)]\n",
      "Training labels: [array([1., 0., 0., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32)]\n",
      "Validation outputs: [0.055793125, 3.5776316e-07, 1.6181016e-07, 4.3024988e-07, 9.411095e-07, 1.457764e-06, 3.2940097e-06, 2.064873e-08, 2.0793031e-07, 3.0308886e-06]\n",
      "Validation labels: [array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([1.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]\n",
      "Validation loss: 27.6400604248, Validation AUC: 0.5016112679\n",
      "--------------------------\n",
      "\n",
      "Epoch: 5/50\n",
      "Training loss: 8597.5293788910, Training AUC: 0.5426451751\n",
      "Training outputs: [array([0.27714995, 0.07356185, 0.2484849 , 0.17210059, 0.22870268],\n",
      "      dtype=float32), array([0.3776525 , 0.03375617, 0.07555996, 0.08381779, 0.42921358],\n",
      "      dtype=float32), array([3.1547859e-08, 9.9998558e-01, 5.9463552e-08, 1.4308203e-05,\n",
      "       3.1181443e-08], dtype=float32), array([0.10653631, 0.3306798 , 0.19634084, 0.08494204, 0.281501  ],\n",
      "      dtype=float32), array([0.20097208, 0.2169258 , 0.3317637 , 0.11659837, 0.13374013],\n",
      "      dtype=float32), array([1.4953763e-04, 3.8895381e-04, 9.9912578e-01, 1.3465890e-04,\n",
      "       2.0107561e-04], dtype=float32), array([0.29663673, 0.19029996, 0.03944007, 0.11225727, 0.36136597],\n",
      "      dtype=float32), array([0.11179725, 0.10325651, 0.11196722, 0.04406456, 0.6289144 ],\n",
      "      dtype=float32), array([0.22444017, 0.20402932, 0.36801744, 0.11842256, 0.08509053],\n",
      "      dtype=float32), array([0.4941742 , 0.10605635, 0.2597614 , 0.07977142, 0.06023668],\n",
      "      dtype=float32)]\n",
      "Training labels: [array([1., 0., 0., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32)]\n",
      "Validation outputs: [0.9984408, 2.182642e-16, 4.953421e-15, 1.4396577e-10, 3.665879e-12, 1.996348e-13, 3.468344e-11, 1.7909257e-16, 2.5954061e-15, 4.647309e-13]\n",
      "Validation labels: [array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([1.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]\n",
      "Validation loss: 27.8013076782, Validation AUC: 0.5063930604\n",
      "--------------------------\n",
      "\n",
      "Epoch: 6/50\n",
      "Training loss: 8582.6615562439, Training AUC: 0.5492100769\n",
      "Training outputs: [array([4.4973497e-03, 4.0033818e-04, 9.8636222e-01, 5.3726016e-03,\n",
      "       3.3675949e-03], dtype=float32), array([8.7365238e-03, 3.1525778e-05, 1.1338413e-04, 9.8512419e-06,\n",
      "       9.9110866e-01], dtype=float32), array([9.0717356e-27, 1.0000000e+00, 2.0125298e-26, 2.6913937e-22,\n",
      "       7.3833967e-29], dtype=float32), array([6.5069329e-03, 8.9660472e-01, 7.5858623e-02, 6.5199209e-05,\n",
      "       2.0964531e-02], dtype=float32), array([0.00234395, 0.02172896, 0.90303713, 0.03410826, 0.03878175],\n",
      "      dtype=float32), array([1.2492420e-17, 3.7720911e-14, 1.0000000e+00, 4.6391275e-16,\n",
      "       5.0509819e-16], dtype=float32), array([9.0149242e-01, 8.7647334e-02, 1.9304364e-04, 4.2258968e-04,\n",
      "       1.0244623e-02], dtype=float32), array([0.13263498, 0.26744395, 0.31109726, 0.00747531, 0.2813485 ],\n",
      "      dtype=float32), array([0.3086641 , 0.01972681, 0.6680835 , 0.0011628 , 0.00236279],\n",
      "      dtype=float32), array([0.04502492, 0.00456946, 0.9312381 , 0.00578667, 0.01338084],\n",
      "      dtype=float32)]\n",
      "Training labels: [array([1., 0., 0., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32)]\n",
      "Validation outputs: [1.0, 0.09343098, 1.0, 1.0, 1.0, 0.99878055, 0.9999995, 0.07099227, 0.9994235, 0.8971259]\n",
      "Validation labels: [array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([1.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]\n",
      "Validation loss: 26.8108329773, Validation AUC: 0.5035102482\n",
      "--------------------------\n",
      "\n",
      "Epoch: 7/50\n",
      "Training loss: 8525.5801525116, Training AUC: 0.5834222886\n",
      "Training outputs: [array([9.4244831e-05, 9.0117455e-06, 9.9965262e-01, 1.4357266e-04,\n",
      "       1.0057399e-04], dtype=float32), array([5.0682861e-02, 3.9388181e-04, 2.9077639e-03, 9.1988605e-04,\n",
      "       9.4509560e-01], dtype=float32), array([0.0000000e+00, 9.9999499e-01, 0.0000000e+00, 4.9734326e-06,\n",
      "       0.0000000e+00], dtype=float32), array([4.7030443e-04, 2.6111969e-01, 7.1474135e-01, 7.5163496e-07,\n",
      "       2.3667896e-02], dtype=float32), array([0.13808699, 0.01377359, 0.8432805 , 0.0025279 , 0.00233097],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([5.3230882e-02, 9.4079542e-01, 2.7456363e-05, 5.8922395e-03,\n",
      "       5.4025328e-05], dtype=float32), array([6.9568545e-02, 3.4873106e-03, 4.6250978e-03, 1.1200841e-05,\n",
      "       9.2230791e-01], dtype=float32), array([3.0786181e-03, 3.4600135e-04, 6.6261007e-03, 9.8994923e-01,\n",
      "       8.4331724e-08], dtype=float32), array([1.2591395e-19, 1.5802859e-18, 1.0000000e+00, 6.6077462e-19,\n",
      "       2.3460745e-18], dtype=float32)]\n",
      "Training labels: [array([1., 0., 0., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32)]\n",
      "Validation outputs: [0.9999987, 0.9759975, 0.9971955, 0.99992836, 0.99980086, 0.9998148, 0.99999964, 0.9976803, 0.99997723, 0.9991493]\n",
      "Validation labels: [array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([1.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32)]\n",
      "Validation loss: 26.7951049805, Validation AUC: 0.4995947745\n",
      "--------------------------\n",
      "\n",
      "Epoch: 8/50\n",
      "Training loss: 8436.1901779175, Training AUC: 0.6090414176\n",
      "Training outputs: [array([0.01106746, 0.01007811, 0.94529337, 0.02607925, 0.00748189],\n",
      "      dtype=float32), array([4.3093967e-03, 1.5226320e-03, 9.3115779e-04, 7.3757023e-04,\n",
      "       9.9249929e-01], dtype=float32), array([1.8068903e-09, 1.0000000e+00, 2.0558873e-10, 2.6987404e-10,\n",
      "       5.7872145e-12], dtype=float32), array([0.00762997, 0.2374656 , 0.6916159 , 0.0020418 , 0.06124682],\n",
      "      dtype=float32), array([0.00783071, 0.00943078, 0.9423373 , 0.03231087, 0.00809031],\n",
      "      dtype=float32), array([4.2191811e-08, 3.6833480e-05, 9.9994779e-01, 1.3699211e-05,\n",
      "       1.6435314e-06], dtype=float32), array([0.21445125, 0.7628461 , 0.00625797, 0.00890548, 0.00753918],\n",
      "      dtype=float32), array([0.0333253 , 0.05804942, 0.76072025, 0.01504167, 0.13286346],\n",
      "      dtype=float32), array([8.9321762e-02, 2.9469690e-01, 6.0977948e-01, 5.8784164e-03,\n",
      "       3.2345310e-04], dtype=float32), array([0.07975172, 0.02321254, 0.01099502, 0.01048884, 0.8755519 ],\n",
      "      dtype=float32)]\n",
      "Training labels: [array([1., 0., 0., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 1., 0., 0.], dtype=float32)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m his_input_title, pred_input_title, labels, outputs\n\u001b[1;32m---> 99\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Calculate AUC score\u001b[39;00m\n\u001b[0;32m    102\u001b[0m auc \u001b[38;5;241m=\u001b[39m roc_auc_score(all_labels, all_outputs)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearningNew\\Lib\\site-packages\\torch\\cuda\\memory.py:192\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.utils  # Ensure this is imported for gradient clipping\n",
    "\n",
    "epoch = 0\n",
    "num_epochs = hparams_nrms.epochs\n",
    "\n",
    "word2vec_embedding = torch.tensor(word2vec_embedding, dtype=torch.float32).to(device)\n",
    "\n",
    "nrms = NRMSModel(hparams_nrms=hparams_nrms, word2vec_embedding=word2vec_embedding, seed=50).to(device)  # Adding to device\n",
    "print(nrms)\n",
    "\n",
    "for name, param in nrms.named_parameters():\n",
    "    print(f\"Parameter: {name}, Requires Grad: {param.requires_grad}, Shape: {param.shape}\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(nrms.parameters(), lr=hparams_nrms.learning_rate, weight_decay=hparams_nrms.weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "val_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Gradient clipping parameter\n",
    "max_norm = 5.0  # Maximum gradient norm\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    nrms.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    for i, ((his_input_title, pred_input_title), labels) in enumerate(train_dataloader):\n",
    "        his_input_title = his_input_title.to(device, dtype=torch.long)\n",
    "        pred_input_title = pred_input_title.to(device, dtype=torch.long)\n",
    "        og_labels = labels\n",
    "        labels = labels.to(device, dtype=torch.long).view(-1)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = nrms(his_input_title, pred_input_title).to(device)  # Forward pass\n",
    "        \n",
    "        loss = loss_fn(outputs.view(-1), labels.float())  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        loss.detach()  # Detach the loss to save memory\n",
    "        \n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(nrms.parameters(), max_norm)\n",
    "\n",
    "        optimizer.step()  # Update the parameters\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "        \n",
    "        # Detach tensors immediately after use to save memory\n",
    "        his_input_title.detach()\n",
    "        pred_input_title.detach()\n",
    "        labels.detach()\n",
    "        \n",
    "        # Save labels and outputs for AUC calculation\n",
    "        all_labels.extend(og_labels.detach().cpu().numpy())\n",
    "        all_outputs.extend(outputs.detach().cpu().numpy())\n",
    "        \n",
    "        del his_input_title, pred_input_title, labels, loss, outputs, og_labels\n",
    "        torch.cuda.empty_cache()  # Clear unused GPU memory\n",
    "\n",
    "    # Calculate AUC score\n",
    "    auc = 0\n",
    "    for i, label_true in enumerate(all_labels):\n",
    "        auc += roc_auc_score(label_true, all_outputs[i])\n",
    "    auc /= len(all_labels)\n",
    "    \n",
    "    # Print training details\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Training loss: {running_loss:.10f}, Training AUC: {auc:.10f}\")\n",
    "    print(f\"Training outputs: {all_outputs[:10]}\")\n",
    "    print(f\"Training labels: {all_labels[:10]}\")\n",
    "\n",
    "    # Validation loop\n",
    "    nrms.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, ((his_input_title, pred_input_title), labels) in enumerate(val_dataloader):\n",
    "            his_input_title = his_input_title.to(device, dtype=torch.long)\n",
    "            pred_input_title = pred_input_title.to(device, dtype=torch.long)\n",
    "            og_labels = labels\n",
    "            labels = labels.to(device, dtype=torch.long).view(-1)\n",
    "\n",
    "            outputs = nrms(his_input_title, pred_input_title).to(device)  # Forward pass\n",
    "            loss = val_loss_fn(outputs.view(-1), labels.float())\n",
    "            val_loss = loss.item()\n",
    "\n",
    "            all_labels.extend(og_labels.cpu().numpy())\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            \n",
    "            # Detach tensors immediately after use to save memory\n",
    "            his_input_title = his_input_title.detach()\n",
    "            pred_input_title = pred_input_title.detach()\n",
    "            labels = labels.detach()\n",
    "            outputs = outputs.detach()\n",
    "            del his_input_title, pred_input_title, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(all_labels, all_outputs)\n",
    "    \n",
    "    print(f\"Validation outputs: {all_outputs[:10]}\")\n",
    "    print(f\"Validation labels: {all_labels[:10]}\")\n",
    "    \n",
    "    # Print validation details\n",
    "    print(f\"Validation loss: {val_loss:.10f}, Validation AUC: {auc:.10f}\")\n",
    "    print(f\"--------------------------\\n\")\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"NRMS\"\n",
    "# LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "# MODEL_WEIGHTS = f\"downloads/data/state_dict/{MODEL_NAME}/weights\"\n",
    "\n",
    "# # CALLBACKS\n",
    "# # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "# # early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "# # modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "# #     filepath=MODEL_WEIGHTS, save_best_only=True, save_weights_only=True, verbose=1\n",
    "# # )\n",
    "\n",
    "# hparams_nrms.history_size = HISTORY_SIZE\n",
    "\n",
    "\n",
    "# model = NRMSModel(\n",
    "#     hparams=hparams_nrms,\n",
    "#     word2vec_embedding=word2vec_embedding,\n",
    "#     seed=42,\n",
    "# )\n",
    "# hist = model.model.fit(\n",
    "#     train_dataloader,\n",
    "#     validation_data=val_dataloader,\n",
    "#     epochs=1,\n",
    "#     # callbacks=[tensorboard_callback, early_stopping, modelcheckpoint],\n",
    "# )\n",
    "# Uncomment the following line if you have pre-trained weights\n",
    "# _ = model.model.load_weights(filepath=MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_validation = model.scorer.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the predictions to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "#     add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    "# )\n",
    "# df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = MetricEvaluator(\n",
    "#     labels=df_validation[\"labels\"].to_list(),\n",
    "#     predictions=df_validation[\"scores\"].to_list(),\n",
    "#     metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    "# )\n",
    "# metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_validation = df_validation.with_columns(\n",
    "#     pl.col(\"scores\")\n",
    "#     .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "#     .alias(\"ranked_scores\")\n",
    "# )\n",
    "# df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the validation, simply add the testset to your flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_submission_file(\n",
    "#     impression_ids=df_validation[DEFAULT_IMPRESSION_ID_COL],\n",
    "#     prediction_scores=df_validation[\"ranked_scores\"],\n",
    "#     path=\"downloads/predictions.txt\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningNew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
